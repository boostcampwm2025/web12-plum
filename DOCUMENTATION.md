# MediaPipe 활용 제스처 인식 기능 기술 검증(PoC) 보고서

## 1. 기술 검증 개요

### 1.1. 목표
본 기술 검증의 핵심 목표는, 우리 프로젝트에 도입할 실시간 손동작 인식 기능의 구현 가능성을 **Google의 MediaPipe 라이브러리**를 통해 사전에 빠르고 정확하게 확인하는 것이었습니다.

### 1.2. 검증 항목
- **핵심 기능**: 웹캠 환경에서 실시간으로 손의 위치와 모양을 안정적으로 추적할 수 있는가?
- **확장성**: 우리가 정의한 커스텀 제스처(OK 사인, 숫자 등)를 인식하도록 기능을 확장할 수 있는가?
- **사용자 경험**: 오인식을 줄이고 안정적인 인식을 위해 특정 매커니즘(예: 2초간 자세 유지)을 도입할 수 있는가?
- **성능**: 일반적인 사용자 PC 환경에서 CPU에 과도한 부담을 주지 않고 부드럽게 동작하는가?

### 1.3. 결론 요약
**MediaPipe는 우리 프로젝트의 제스처 인식 요구사항을 충분히 만족시키는 매우 실용적이고 강력한 도구임을 확인했습니다.** MediaPipe의 기본 인식 기능과 우리가 직접 작성한 '규칙 기반' 로직을 결합하여, 목표로 했던 대부분의 커스텀 제스처를 성공적으로 인식시켰습니다.

---

## 2. 제스처 인식 기능 구현 방식

제스처 인식 기능은 두 가지 핵심 기술의 조합으로 구현되었습니다.

### 2.1. 접근법 1: MediaPipe의 기본 제스처 분석기 활용
MediaPipe의 `GestureRecognizer`는 사전 훈련된 머신러닝 모델을 통해, 별도의 학습 과정 없이도 즉시 사용할 수 있는 강력한 기능을 제공합니다.

- **어떻게 동작하는가?**: 웹캠 영상에서 실시간으로 손을 찾아내고, 각 손마다 **21개의 주요 랜드마크(뼈마디)**의 3D 좌표를 정확하게 추출합니다.
- **기본 제공 제스처**: 이 랜드마크 정보를 기반으로, `Thumb_Up`(엄지 올리기), `Thumb_Down`(엄지 내리기), `Open_Palm`(손바닥 펴기) 등 7~8가지의 기본적인 제스처를 자동으로 인식하고 분류해줍니다.

### 2.2. 접근법 2: '규칙 기반' 커스텀 제스처 분류기 구현
MediaPipe의 기본 모델은 편리하지만, 우리가 원하는 'OK 사인'이나 '숫자 1~4'와 같은 세부적인 제스처는 인식하지 못합니다. 이 한계를 극복하기 위해, 우리는 **규칙 기반(Rule-Based) 분류기**를 직접 코드로 구현했습니다.

- **어떻게 구현했는가?**: `gestures.js` 라는 파일을 만들어, MediaPipe가 제공하는 21개 랜드마크의 **좌표 데이터를 직접 분석**하는 로직을 작성했습니다.
- **규칙 예시 ("OK 사인")**: "엄지손가락 끝(4번 랜드마크)과 집게손가락 끝(8번 랜드마크)의 거리가 매우 가깝고, 나머지 세 손가락은 모두 펴져 있다" 와 같은 **기하학적 조건을 코드로 정의**했습니다.
- **장점**: 이 방식을 통해, 새로운 머신러닝 모델을 훈련시키는 복잡하고 비용이 많이 드는 과정 없이도, 우리가 원하는 새로운 제스처를 빠르고 유연하게 추가할 수 있음을 확인했습니다.

---

## 3. 안정적인 인식을 위한 매커니즘

단순히 제스처를 인식하는 것을 넘어, 사용자가 의도한 행동만 정확히 감지하기 위한 두 가지 주요 매커니즘을 도입했습니다.

### 3.1. '2초 유지' 확정 로직
- **문제점**: 사용자가 의도치 않게 잠시 취한 동작이나, 한 제스처에서 다른 제스처로 바뀌는 중간 과정을 잘못 인식할 수 있습니다.
- **해결책**: 특정 제스처가 감지되면 즉시 '인식'으로 판단하지 않고, **2초 동안 동일한 제스처가 안정적으로 유지되었을 때** 비로소 "확정된 제스처"로 판단하는 로직을 추가했습니다.
- **사용자 피드백**: 사용자가 제스처를 유지하는 동안, 화면 하단에 **아이콘과 진행률 표시줄**이 나타나 직관적으로 인식 과정을 알 수 있도록 하여 사용성을 높였습니다.

### 3.2. 성능 최적화
- **문제점**: 실시간 영상 분석은 CPU에 큰 부담을 줄 수 있습니다.
- **해결책 (Throttling)**: 가장 무거운 제스처 '인식' 연산은 **0.3초(300ms)에 한 번**만 수행하도록 제한했습니다. 반면, 화면에 랜드마크를 그리고 UI를 업데이트하는 가벼운 작업은 매 프레임마다 처리하여, 사용자에게는 끊김 없는 부드러운 화면을 제공하면서도 시스템의 부담은 줄였습니다.

---

## 4. 기술 검증 결과

### 4.1. 인식에 성공한 제스처 목록
이번 기술 검증을 통해 아래 제스처들을 안정적으로 인식하는 데 성공했습니다.

| 제스처 이름    | 영문 ID        | 구현 방식        |
|:----------|:-------------|:-------------|
| 엄지 올리기    | `Thumb_Up`   | MediaPipe 기본 |
| 엄지 내리기    | `Thumb_Down` | MediaPipe 기본 |
| 손 들기      | `Open_Palm`  | MediaPipe 기본 |
| **OK 사인** | `ok_sign`    | **커스텀 규칙**   |
| **숫자 1**  | `number_1`   | **커스텀 규칙**   |
| **숫자 2**  | `number_2`   | **커스텀 규칙**   |
| **숫자 3**  | `number_3`   | **커스텀 규칙**   |
| **숫자 4**  | `number_4`   | **커스텀 규칙**   |

### 4.2. 현재 구조에서 구현이 어려운 제스처
- **'O 사인 (머리 위로 원 그리기)'**: 이 동작은 손뿐만 아니라 팔과 머리의 위치 정보가 필요합니다. 현재 사용한 `GestureRecognizer`는 **오직 손만 추적**하므로 구현이 불가능합니다. 이 기능을 위해서는 전신 포즈를 추적하는 `PoseLandmarker`와 같은 별도의 도구가 필요합니다.
- **'X 사인 (두 손으로 X 만들기)'**: 두 손의 위치와 교차 상태를 안정적으로 판단하는 규칙을 만드는 것은 복잡성이 매우 높아 이번 검증 범위에서는 제외했습니다.

### 4.3. 최종 결론
MediaPipe는 웹 환경에서 실시간 제스처 인식을 구현하는 데 매우 효과적이고 유연한 솔루션입니다. 특히 **기본 모델의 한계를 '규칙 기반 로직'으로 보완**할 수 있어, 우리 프로젝트의 요구사항에 맞게 기능을 확장할 수 있는 잠재력이 매우 높다고 판단됩니다.